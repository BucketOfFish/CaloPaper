\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces DNN hyperparameter scan over number of hidden layers and number of neurons per layer. Decay rate is held fixed at 0.001, and dropout rate is held at 0.1. (A) shows a learning rate of 0.001, (B) shows a learning rate of 0.01, and (C) shows a learning rate of 0.1.}}{2}}
\newlabel{DNNscan1}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces DNN hyperparameter scan over decay rate and dropout rate. Learning rate is held fixed at <X>, number of hidden layers is fixed at <Y>, and neurons per hidden layer is fixed at <Z>.}}{2}}
\newlabel{DNNscan2}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ROC for DNN trained at best hyperparameter point.}}{3}}
\newlabel{DNNROC_photon}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Loss history for training and test samples.}}{3}}
\newlabel{DNNtrain_photon}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces ROC for DNN trained at best hyperparameter point.}}{4}}
\newlabel{DNNROC_electron}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Loss history for training and test samples.}}{4}}
\newlabel{DNNtrain_electron}{{6}{4}}
