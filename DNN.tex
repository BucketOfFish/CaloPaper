\documentclass{article}
\usepackage{times}
\usepackage{graphicx}

\begin{document}

    \title{Technical Details on Neural Net Analysis of Calorimeter Images}
    \author{Matt Zhang, Wei Wei}
    \date{}
    \maketitle

    \section*{DNN}

    We have found good performance results from using a dense neural network (DNN) on ECAL and HCAL slices. Starting with a 25x25x25 ECAL window and a 5x5x60 HCAL window, we first begin by flattening both calorimeter slices into a single one-dimensional input array. A rectified linear unit (relu) is applied directly to the input. (EDIT: this makes no sense and was a mistake. I should rerun the hyperparameter scan without this, and also add dropout to the input layer). Following this, we place X hidden layers with Y neurons each, where X and Y are optimized with a hyperparameter scan. Each layer is followed by a relu function and a dropout layer. The dropout rate is also determined via a hyperparameter scan.

    \section*{Hyperparameter Scan}

    We run a hyperparameter scan over the following variables. The results of these scans are shown in Figure~\ref{DNNscan1} and~\ref{DNNscan2}.
 
    \begin{itemize}
        \item number of hidden layers, between one and five
        \item number of neurons in each hidden layer, either 10, 30, or 50
        \item dropout rate, either 10\%, 30\%, or 50\%
        \item learning rate, either 0.001, 0.01, or 0.1
        \item learning decay rate, either 0.001 or 0.01
    \end{itemize}

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{DNN hyperparameter scan over number of hidden layers and number of neurons per layer. Decay rate is held fixed at 0.001, and dropout rate is held at 0.1. (A) shows a learning rate of 0.001, (B) shows a learning rate of 0.01, and (C) shows a learning rate of 0.1.}
        \label{DNNscan1}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{DNN hyperparameter scan over decay rate and dropout rate. Learning rate is held fixed at <X>, number of hidden layers is fixed at <Y>, and neurons per hidden layer is fixed at <Z>.}
        \label{DNNscan2}
    \end{figure}

    \subsection*{Photon vs. Neutral Pion}

    The ROC for the best hyperparmeter point at <point> is shown in Figure~\ref{DNNROC_photon}, and the loss curves for both training and test samples at that point is shown in Figure~\ref{DNNtrain_photon}.

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{ROC for DNN trained at best hyperparameter point.}
        \label{DNNROC_photon}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{Loss history for training and test samples.}
        \label{DNNtrain_photon}
    \end{figure}

    \subsection*{Electron vs. Charged Pion}

    The ROC for the best hyperparmeter point at <point> is shown in Figure~\ref{DNNROC_electron}, and the loss curves for both training and test samples at that point is shown in Figure~\ref{DNNtrain_electron}.

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{ROC for DNN trained at best hyperparameter point.}
        \label{DNNROC_electron}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics{images/cat-thumb.jpeg}
        \end{center}
        \caption{Loss history for training and test samples.}
        \label{DNNtrain_electron}
    \end{figure}

\end{document}
